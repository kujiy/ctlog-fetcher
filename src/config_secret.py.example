# config.py
import os
import datetime
from datetime import timedelta, timezone

from dotenv import load_dotenv
load_dotenv()

# JST timezone
JST = timezone(timedelta(hours=9))
BATCH_SIZE = 16000

MANAGER_API_URL = os.getenv('MANAGER_API_URL', 'http://127.0.0.1:1173')
DASHBOARD_URL = os.getenv('DASHBOARD_URL', 'http://127.0.0.1:1174')

# Import MYSQL_URL from config_secret.py
try:
    from .config_secret import MYSQL_URL
except ImportError:
    # Fallback if config_secret.py doesn't exist
    MYSQL_URL = os.getenv('MYSQL_URL', 'mysql+aiomysql://root@127.0.0.1:3306/ct')
MANAGER_API_URL_FOR_UI = os.environ.get("MANAGER_API_URL_FOR_UI", "http://127.0.0.1:1173")
METRICS_URL = os.getenv('METRICS_URL', MANAGER_API_URL_FOR_UI)
BACKGROUND_JOBS_ENABLED = os.getenv('BACKGROUND_JOBS_ENABLED', 'true').lower() == 'true'

ETA_BASE_DATE = datetime.date(2025, 8, 24)

# get_next_task increases the CPU usage if this is too big because get_next_task start finding the new job from LogFetchProgress.
LOG_FETCH_PROGRESS_TTL = 60

"""
# Interval (seconds) at which the LogFetchProgress aggregation job runs
When the growing log_name is almost complete, a task is generated to duplicate a job for the last 16,000< entries
 and retrieve them over and over again. This is because start is always fixed to a multiple of batch_size.
 Therefore, this value should be set to a value larger than the time it takes for a worker and CT Log to process/increase 16,000 entries.
"""
STH_FETCH_INTERVAL_SEC = 60 * 30  # n mins

"""
# Interval (seconds) at which the worker accesses the CT log
If this value is set too high, the worker will be mostly idle, and instructions from the manager API will not be reflected to the worker.
Like communicating with a spaceship, it is desirable to set the interval to several minutes to several tens of minutes at most, so that communication can occur at a reasonable frequency.
"""
WORKER_CTLOG_REQUEST_INTERVAL_SEC = 1
WORKER_PING_INTERVAL_SEC = 300  # a worker sends ping every n seconds

## liveness
# set this smaller, workers are marked as dead faster, the job is reassigned faster, LogFetchProgress bigger, get_next_task faster
WORKER_LIVENESS_TTL = 120
WORKER_DEAD_THRESHOLD_MINS = 60  # API marks the worker as dead if no ping for n minutes; The API is going to assign its tasks to other workers then
assert WORKER_PING_INTERVAL_SEC < WORKER_DEAD_THRESHOLD_MINS * 60, "WORKER_PING_INTERVAL_SEC must be less than WORKER_DEAD_THRESHOLD_MINS * 60"

WORKER_THREAD_MANAGER_INTERVAL_SEC = 600  # category check interval

# Total maximum number of worker threads for preventing DDoS attack against CT log APIs
MAX_THREADS_PER_WORKER = 6
MIN_THREADS_PER_WORKER = 1
DDOS_ADJUST_INTERVAL_MINUTES = 10
MAX_COMPLETED_JOBS_PER_DDOS_ADJUST_INTERVAL = 2200

# Which categories to fetch, in order of priority
ORDERED_CATEGORIES = [
    "google", "google", "google",
    "digicert",
    "cloudflare",
    "letsencrypt",
    "trustasia",
    # "sectigo"
]

CT_LOG_ENDPOINTS = {
    "cloudflare": [
        ("nimbus2025", "https://ct.cloudflare.com/logs/nimbus2025/"),
        ("nimbus2026", "https://ct.cloudflare.com/logs/nimbus2026/")
    ],
    "google": [
        ("argon2022", "https://ct.googleapis.com/logs/argon2022/"),
        ("argon2023", "https://ct.googleapis.com/logs/argon2023/"),
        ("argon2024", "https://ct.googleapis.com/logs/us1/argon2024/"),
        ("argon2025h1", "https://ct.googleapis.com/logs/us1/argon2025h1/"),
        ("argon2025h2", "https://ct.googleapis.com/logs/us1/argon2025h2/"),
        ("argon2026h1", "https://ct.googleapis.com/logs/us1/argon2026h1/"),
        ("argon2026h2", "https://ct.googleapis.com/logs/us1/argon2026h2/"),
        ("xenon2022", "https://ct.googleapis.com/logs/xenon2022/"),
        ("xenon2023", "https://ct.googleapis.com/logs/xenon2023/"),
        ("xenon2024", "https://ct.googleapis.com/logs/eu1/xenon2024/"),
        ("xenon2025h1", "https://ct.googleapis.com/logs/eu1/xenon2025h1/"),
        ("xenon2025h2", "https://ct.googleapis.com/logs/eu1/xenon2025h2/"),
        ("xenon2026h1", "https://ct.googleapis.com/logs/eu1/xenon2026h1/"),
        ("xenon2026h2", "https://ct.googleapis.com/logs/eu1/xenon2026h2/"),
    ],
    "trustasia": [
        ("log2024", "https://ct2024.trustasia.com/log2024/"),
        ("log2025a", "https://ct2025-a.trustasia.com/log2025a/"),
        ("log2025b", "https://ct2025-b.trustasia.com/log2025b/"),
        ("log2026a", "https://ct2026-a.trustasia.com/log2026a/"),
        ("log2026b", "https://ct2026-b.trustasia.com/log2026b/")
    ],
    "digicert": [
        ("nessie2025", "https://nessie2025.ct.digicert.com/log/"),
        ("sphinx2024h1", "https://sphinx.ct.digicert.com/2024h1/"),
        ("sphinx2024h2", "https://sphinx.ct.digicert.com/2024h2/"),
        ("sphinx2025h1", "https://sphinx.ct.digicert.com/2025h1/"),
        ("sphinx2025h2", "https://sphinx.ct.digicert.com/2025h2/"),
        ("sphinx2026h1", "https://sphinx.ct.digicert.com/2026h1/"),
        ("sphinx2026h2", "https://sphinx.ct.digicert.com/2026h2/"),
        ("wyvern2024h1", "https://wyvern.ct.digicert.com/2024h1/"),
        ("wyvern2024h2", "https://wyvern.ct.digicert.com/2024h2/"),
        ("wyvern2025h1", "https://wyvern.ct.digicert.com/2025h1/"),
        ("wyvern2025h2", "https://wyvern.ct.digicert.com/2025h2/"),
        ("wyvern2026h1", "https://wyvern.ct.digicert.com/2026h1/"),
        ("wyvern2026h2", "https://wyvern.ct.digicert.com/2026h2/"),
    ],
    "letsencrypt": [
        ("2025h1", "https://oak.ct.letsencrypt.org/2025h1/"),
        ("2025h2", "https://oak.ct.letsencrypt.org/2025h2/"),
        ("2026h1", "https://oak.ct.letsencrypt.org/2026h1/"),
        ("2026h2", "https://oak.ct.letsencrypt.org/2026h2/")
    ],
    # "sectigo": [
    #     ("sabre2024h1", "https://sabre2024h1.ct.sectigo.com/"),
    #     ("sabre2024h2", "https://sabre2024h2.ct.sectigo.com/"),
    #     ("sabre2025h1", "https://sabre2025h1.ct.sectigo.com/"),
    #     ("sabre2025h2", "https://sabre2025h2.ct.sectigo.com/"),
    #     ("mammoth2024h1", "https://mammoth2024h1.ct.sectigo.com/"),
    #     ("mammoth2024h1b", "https://mammoth2024h1b.ct.sectigo.com/"),
    #     ("mammoth2024h2", "https://mammoth2024h2.ct.sectigo.com/"),
    #     ("mammoth2025h1", "https://mammoth2025h1.ct.sectigo.com/"),
    #     ("mammoth2025h2", "https://mammoth2025h2.ct.sectigo.com/"),
    #     ("mammoth2026h1", "https://mammoth2026h1.ct.sectigo.com/"),
    #     ("mammoth2026h2", "https://mammoth2026h2.ct.sectigo.com/"),
    #     ("sabre2026h1", "https://sabre2026h1.ct.sectigo.com/"),
    #     ("sabre2026h2", "https://sabre2026h2.ct.sectigo.com/"),
    #     ("elephant2025h2", "https://elephant2025h2.ct.sectigo.com/"),
    #     ("elephant2026h1", "https://elephant2026h1.ct.sectigo.com/"),
    #     ("elephant2026h2", "https://elephant2026h2.ct.sectigo.com/"),
    #     ("elephant2027h1", "https://elephant2027h1.ct.sectigo.com/"),
    #     ("elephant2027h2", "https://elephant2027h2.ct.sectigo.com/"),
    #     ("tiger2025h2", "https://tiger2025h2.ct.sectigo.com/"),
    #     ("tiger2026h1", "https://tiger2026h1.ct.sectigo.com/"),
    #     ("tiger2026h2", "https://tiger2026h2.ct.sectigo.com/"),
    #     ("tiger2027h1", "https://tiger2027h1.ct.sectigo.com/"),
    #     ("tiger2027h2", "https://tiger2027h2.ct.sectigo.com/"),
    # ]
}
